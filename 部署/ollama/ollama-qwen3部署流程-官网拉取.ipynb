{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source /etc/network_turbo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置ollama环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cpu 加载/ gpu 加载 （自动识别）\n",
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:6006\" \n",
    "\n",
    "export OLLAMA_MODELS=/root/autodl-tmp/models \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU加载 单卡/多卡\n",
    "\n",
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:6006\" \n",
    "\n",
    "export OLLAMA_GPU_LAYER=cuda\n",
    "\n",
    "export OLLAMA_NUM_GPU=2\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=0,1\n",
    "\n",
    "export OLLAMA_SCHED_SPREAD=1\n",
    "\n",
    "export OLLAMA_KEEP_ALIVE=-1\n",
    "\n",
    "export OLLAMA_MODELS=/root/autodl-tmp/models \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开启ollama服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从官方拉取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama run qwen3:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本地openai调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "哈哈，来一个笑话吧！\n",
      "\n",
      "有一天，乌龟和兔子赛跑，兔子赢了。  \n",
      "乌龟不服气地说：“我再跑一次，这次你可别偷懒了！”  \n",
      "兔子笑着说：“你先说好，这次我跑得慢一点，但你得跑得快一点哦！”  \n",
      "乌龟想了想，说：“行啊，那我先跑一百步，你跑一百步，我们再比一次！”  \n",
      "兔子一听，直接躺平了：“你这乌龟，还是别跑了，我直接认输吧！”  \n",
      "\n",
      "😂 你猜为啥？因为乌龟是“龟速”跑，兔子是“兔速”跑，根本没法比啊！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#     # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "#     api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "#     base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "# )\n",
    "\n",
    "client = OpenAI(\n",
    "   \n",
    "    api_key=\"na\", \n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen3:8b\", \n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': '讲个笑话 /no_think'}\n",
    "        ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过open-webui 部署ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:11434\" \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意，不需要以下环节\n",
    "export HF_ENDPOINT=https://hf-mirror.com \\\n",
    "export ENABLE_OLLAMA_API=False \\\n",
    "export OPENAI_API_BASE_URL=http://127.0.0.1:5000/v1 \\\n",
    "export DEFAULT_MODELS=\"Qwen3-8B\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接启动open-webui即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama serve\n",
    "\n",
    "open-webui serve --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
