{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‹è½½ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source /etc/network_turbo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é…ç½®ollamaç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cpu åŠ è½½/ gpu åŠ è½½ ï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼‰\n",
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:6006\" \n",
    "\n",
    "export OLLAMA_MODELS=/root/autodl-tmp/models \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUåŠ è½½ å•å¡/å¤šå¡\n",
    "\n",
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:6006\" \n",
    "\n",
    "export OLLAMA_GPU_LAYER=cuda\n",
    "\n",
    "export OLLAMA_NUM_GPU=2\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=0,1\n",
    "\n",
    "export OLLAMA_SCHED_SPREAD=1\n",
    "\n",
    "export OLLAMA_KEEP_ALIVE=-1\n",
    "\n",
    "export OLLAMA_MODELS=/root/autodl-tmp/models \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¼€å¯ollamaæœåŠ¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä»å®˜æ–¹æ‹‰å–æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama run qwen3:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æœ¬åœ°openaiè°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "å“ˆå“ˆï¼Œæ¥ä¸€ä¸ªç¬‘è¯å§ï¼\n",
      "\n",
      "æœ‰ä¸€å¤©ï¼Œä¹Œé¾Ÿå’Œå…”å­èµ›è·‘ï¼Œå…”å­èµ¢äº†ã€‚  \n",
      "ä¹Œé¾Ÿä¸æœæ°”åœ°è¯´ï¼šâ€œæˆ‘å†è·‘ä¸€æ¬¡ï¼Œè¿™æ¬¡ä½ å¯åˆ«å·æ‡’äº†ï¼â€  \n",
      "å…”å­ç¬‘ç€è¯´ï¼šâ€œä½ å…ˆè¯´å¥½ï¼Œè¿™æ¬¡æˆ‘è·‘å¾—æ…¢ä¸€ç‚¹ï¼Œä½†ä½ å¾—è·‘å¾—å¿«ä¸€ç‚¹å“¦ï¼â€  \n",
      "ä¹Œé¾Ÿæƒ³äº†æƒ³ï¼Œè¯´ï¼šâ€œè¡Œå•Šï¼Œé‚£æˆ‘å…ˆè·‘ä¸€ç™¾æ­¥ï¼Œä½ è·‘ä¸€ç™¾æ­¥ï¼Œæˆ‘ä»¬å†æ¯”ä¸€æ¬¡ï¼â€  \n",
      "å…”å­ä¸€å¬ï¼Œç›´æ¥èººå¹³äº†ï¼šâ€œä½ è¿™ä¹Œé¾Ÿï¼Œè¿˜æ˜¯åˆ«è·‘äº†ï¼Œæˆ‘ç›´æ¥è®¤è¾“å§ï¼â€  \n",
      "\n",
      "ğŸ˜‚ ä½ çŒœä¸ºå•¥ï¼Ÿå› ä¸ºä¹Œé¾Ÿæ˜¯â€œé¾Ÿé€Ÿâ€è·‘ï¼Œå…”å­æ˜¯â€œå…”é€Ÿâ€è·‘ï¼Œæ ¹æœ¬æ²¡æ³•æ¯”å•Šï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#     # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key=\"sk-xxx\",\n",
    "#     api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # å¦‚ä½•è·å–API Keyï¼šhttps://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "#     base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "# )\n",
    "\n",
    "client = OpenAI(\n",
    "   \n",
    "    api_key=\"na\", \n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen3:8b\", \n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': 'è®²ä¸ªç¬‘è¯ /no_think'}\n",
    "        ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é€šè¿‡open-webui éƒ¨ç½²ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:11434\" \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ³¨æ„ï¼Œä¸éœ€è¦ä»¥ä¸‹ç¯èŠ‚\n",
    "export HF_ENDPOINT=https://hf-mirror.com \\\n",
    "export ENABLE_OLLAMA_API=False \\\n",
    "export OPENAI_API_BASE_URL=http://127.0.0.1:5000/v1 \\\n",
    "export DEFAULT_MODELS=\"Qwen3-8B\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç›´æ¥å¯åŠ¨open-webuiå³å¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama serve\n",
    "\n",
    "open-webui serve --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
