1.准备环境，git clone 下载llamafac。
2.进入llamafac文件夹后下载依赖。
3.llamafactory-cli webui打开webui，需使用隧道攻击挂载到本地。
4.在网页端使用vllm部署时会报错，因为vllm的版本问题，此时关闭进程，在终端输入export DISABLE_VERSION_CHECK=1，再开启，就可以用vllm部署。

5.如何解决端口挂载问题：
进入 /autodl-tmp/LLaMA-Factory/src/llamafactory/webui/interface.py文件
将run_web_ui()和run_web_demo()两个函数中最后一行share=gradio_share改为share=True
再次运行启动webui (记得是在LLaMA-Factory目录下)
终端提示：
Could not create share link. Missing file: /root/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3. 
Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: 
1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64 记得打开梯子和关闭防火墙
2. Rename the downloaded file to: frpc_linux_amd64_v0.3
3. Move the file to this location: /root/.cache/huggingface/gradio/frpc

运行指令
mv frpc_linux_amd64 /root/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3
cd /root/.cache/huggingface/gradio/frpc/
chmod +x frpc_linux_amd64_v0.3 

再次运行启动webui (记得是在LLaMA-Factory目录下)

依然不成功，只能使用隧道工具开启。

